{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "005_EMBEDDING_GLOVE_V2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxrinal/nlp-disaster/blob/master/TP2_CODES/Rinaldi_91825/EMBEDDING_GLOVE_TWITTER_V7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5EhBACOmKy4",
        "colab_type": "text"
      },
      "source": [
        "# Instalo bibliotecas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtAXhekNjnQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip3 install --upgrade 'tensorflow>=2.0.0a0'\n",
        "# !pip3 install --upgrade 'tensorflow-gpu'\n",
        "# ! pip3 install ipython-autotime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFIA3livHjaY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "fcad8e68-db84-4f33-b2ac-7e6512511918"
      },
      "source": [
        "# %%capture\n",
        "# pip3 install -U instala un paquete en la carpeta de usuario y actualiza cuando ya existe\n",
        "# ! pip3 install textblob\n",
        "\n",
        "# ! pip3 install pandas\n",
        "# ! pip3 install nltk\n",
        "# ! pip3 install --upgrade gensim\n",
        "# !pip3 install sklearn\n",
        "# !pip3 install --upgrade 'tensorflow>=2.0.0a0'\n",
        "# !pip3 install --upgrade 'tensorflow-gpu'\n",
        "\n",
        "import multiprocessing\n",
        "# Para utilizar multi core\n",
        "from multiprocessing import  Pool\n",
        "n_cores = multiprocessing.cpu_count()\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import string,re\n",
        "\n",
        "# Para hacer decode de las url\n",
        "import urllib.parse\n",
        "\n",
        "# Para limpiar html entities\n",
        "import html\n",
        "\n",
        "# Importo nltk para lematizar\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "# Init the Wordnet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "%load_ext autotime\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/rinal/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TvVJ6OuSNoB",
        "colab_type": "text"
      },
      "source": [
        "# Importo el dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c8RmljMeBmR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0f6ef64a-60a8-4291-a50d-fdc6dde1a3ab"
      },
      "source": [
        "# csv_charset = 'latin1'\n",
        "csv_charset = 'utf-8'\n",
        "\n",
        "# train = pd.read_csv('https://raw.githubusercontent.com/maxrinal/nlp-disaster/master/train.csv' ,encoding = csv_charset )\n",
        "# test = pd.read_csv('https://raw.githubusercontent.com/maxrinal/nlp-disaster/master/test.csv' ,encoding = csv_charset )\n",
        "\n",
        "\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/maxrinal/nlp-disaster/master/train.csv' )\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/maxrinal/nlp-disaster/master/test.csv' )\n",
        "\n",
        "\n",
        "\n",
        "pd_cv_test_id = pd.read_csv('SET/set_validation_id.csv')\n",
        "\n",
        "# # Tomo el dataframe original\n",
        "# df_clean_train = train.copy()\n",
        "# df_clean_test  = test.copy()\n",
        "\n",
        "list_cv_test_id = pd_cv_test_id.id.to_list()\n",
        "\n",
        "len( list_cv_test_id)\n",
        "\n",
        "# # Tomo el dataframe original\n",
        "# df_clean_train = train.copy()\n",
        "# df_clean_test  = test.copy()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1523"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "stream",
          "text": [
            "time: 1.37 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJVlM0nr1YM5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "e117372a-0168-4218-99e7-fd4f8b966e57"
      },
      "source": [
        "# Usando un una lista de id común a todo el grupo generarenmos un set para crossvalidation que nos permita generar un ensamble\n",
        "df_clean_train    = train[ ~train.id.isin(list_cv_test_id) ].copy()\n",
        "df_clean_test     = train[   train.id.isin(list_cv_test_id) ].copy()\n",
        "# Hago predicciones sobre un subset de train para poder pesar los resultados de este algoritmo para el ensamble\n",
        "df_clean_predict  = train[   train.id.isin(list_cv_test_id) ].copy()\n",
        "\n",
        "# Descomento esta parte cuando quiero trabajar con el set completo para el caso test\n",
        "df_clean_train = train.copy()\n",
        "# Como la red puede tener distintos resultados testeo con una parte del set de entrenamiento\n",
        "df_clean_test     = train[   train.id.isin(list_cv_test_id) ].copy()\n",
        "df_clean_predict = test.copy()\n",
        "\n",
        "print( len(train) , len(df_clean_train) , len(df_clean_test), len(df_clean_predict) ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7613 7613 1523 3263\n",
            "time: 9.55 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUop7cXutRYf",
        "colab_type": "text"
      },
      "source": [
        "# Limpieza del texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug3vWQk-2G45",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0c14b452-531f-4269-fa6b-8d1dfb8639fe"
      },
      "source": [
        "def super_manual_clean(tweet): \n",
        "            \n",
        "    # Special characters\n",
        "    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n",
        "    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
        "    tweet = re.sub(r\"å_\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n",
        "    tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"åÈ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n",
        "    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n",
        "    tweet = re.sub(r\"å¨\", \"\", tweet)\n",
        "    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n",
        "    tweet = re.sub(r\"åÇ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n",
        "    tweet = re.sub(r\"åÀ\", \"\", tweet)\n",
        "    \n",
        "    # Contractions\n",
        "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
        "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
        "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
        "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
        "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
        "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
        "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
        "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
        "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
        "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
        "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
        "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
        "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
        "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
        "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
        "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
        "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
        "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
        "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
        "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
        "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
        "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
        "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
        "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
        "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
        "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
        "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
        "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
        "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
        "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
        "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
        "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
        "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
        "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
        "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
        "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
        "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
        "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
        "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
        "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
        "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
        "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
        "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
        "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
        "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
        "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
        "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
        "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
        "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
        "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
        "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
        "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
        "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
        "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
        "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
        "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
        "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
        "    tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
        "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
        "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
        "    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
        "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
        "    tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
        "    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
        "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
        "    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
        "    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
        "    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n",
        "    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \n",
        "            \n",
        "    # Character entity references\n",
        "    tweet = re.sub(r\"&gt;\", \">\", tweet)\n",
        "    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n",
        "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n",
        "    \n",
        "    # Typos, slang and informal abbreviations\n",
        "    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n",
        "    tweet = re.sub(r\"w/\", \"with\", tweet)\n",
        "    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n",
        "    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n",
        "    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n",
        "    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n",
        "    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n",
        "    tweet = re.sub(r\"<3\", \"love\", tweet)\n",
        "    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n",
        "    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n",
        "    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\n",
        "    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n",
        "    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\n",
        "    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n",
        "    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n",
        "    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n",
        "    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n",
        "    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n",
        "    \n",
        "    # Hashtags and usernames\n",
        "    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n",
        "    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n",
        "    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n",
        "    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n",
        "    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n",
        "    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n",
        "    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n",
        "    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n",
        "    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n",
        "    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n",
        "    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n",
        "    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n",
        "    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n",
        "    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n",
        "    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n",
        "    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n",
        "    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n",
        "    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n",
        "    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n",
        "    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n",
        "    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n",
        "    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n",
        "    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n",
        "    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n",
        "    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n",
        "    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n",
        "    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n",
        "    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\n",
        "    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n",
        "    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n",
        "    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n",
        "    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n",
        "    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n",
        "    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n",
        "    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n",
        "    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n",
        "    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n",
        "    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n",
        "    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\n",
        "    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n",
        "    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n",
        "    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n",
        "    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n",
        "    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n",
        "    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n",
        "    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n",
        "    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n",
        "    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n",
        "    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n",
        "    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n",
        "    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n",
        "    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n",
        "    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n",
        "    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\n",
        "    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n",
        "    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\n",
        "    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\n",
        "    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n",
        "    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\n",
        "    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n",
        "    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n",
        "    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n",
        "    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n",
        "    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)  \n",
        "    tweet = re.sub(r\"UTC2015\", \"UTC 2015\", tweet)\n",
        "    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n",
        "    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\n",
        "    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n",
        "    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n",
        "    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\n",
        "    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n",
        "    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n",
        "    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\n",
        "    tweet = re.sub(r\"OBLITERATION\", \"obliteration\", tweet)\n",
        "    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\n",
        "    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n",
        "    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\n",
        "    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n",
        "    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n",
        "    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n",
        "    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n",
        "    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n",
        "    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\n",
        "    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n",
        "    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\n",
        "    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\n",
        "    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\n",
        "    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\n",
        "    tweet = re.sub(r\"megynkelly\", \"Megyn Kelly\", tweet)\n",
        "    tweet = re.sub(r\"cnewslive\", \"C News Live\", tweet)\n",
        "    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\n",
        "    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\n",
        "    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\n",
        "    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\n",
        "    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n",
        "    tweet = re.sub(r\"cjoyner\", \"Chris Joyner\", tweet)\n",
        "    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n",
        "    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\n",
        "    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\n",
        "    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\n",
        "    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\n",
        "    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\n",
        "    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\n",
        "    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\n",
        "    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\n",
        "    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\n",
        "    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\n",
        "    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\n",
        "    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\n",
        "    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\n",
        "    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\n",
        "    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\n",
        "    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\n",
        "    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\n",
        "    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n",
        "    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n",
        "    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\n",
        "    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\n",
        "    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\n",
        "    tweet = re.sub(r\"S3XLEAK\", \"sex leak\", tweet)\n",
        "    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\n",
        "    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\n",
        "    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\n",
        "    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n",
        "    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\n",
        "    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\n",
        "    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\n",
        "    tweet = re.sub(r\"foodscare\", \"food scare\", tweet)\n",
        "    tweet = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", tweet)\n",
        "    tweet = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", tweet)\n",
        "    tweet = re.sub(r\"GamerGate\", \"Gamer Gate\", tweet)\n",
        "    tweet = re.sub(r\"IHHen\", \"Humanitarian Relief\", tweet)\n",
        "    tweet = re.sub(r\"spinningbot\", \"spinning bot\", tweet)\n",
        "    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\n",
        "    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\n",
        "    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\n",
        "    tweet = re.sub(r\"po_st\", \"po.st\", tweet)\n",
        "    tweet = re.sub(r\"scoopit\", \"scoop.it\", tweet)\n",
        "    tweet = re.sub(r\"UltimaLucha\", \"Ultima Lucha\", tweet)\n",
        "    tweet = re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", tweet)\n",
        "    tweet = re.sub(r\"aria_ahrary\", \"Aria Ahrary\", tweet)\n",
        "    tweet = re.sub(r\"rapidcity\", \"Rapid City\", tweet)\n",
        "    tweet = re.sub(r\"OutBid\", \"outbid\", tweet)\n",
        "    tweet = re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", tweet)\n",
        "    tweet = re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", tweet)\n",
        "    tweet = re.sub(r\"15PM\", \"15 PM\", tweet)\n",
        "    tweet = re.sub(r\"OriginalFunko\", \"Funko\", tweet)\n",
        "    tweet = re.sub(r\"rightwaystan\", \"Richard Tan\", tweet)\n",
        "    tweet = re.sub(r\"CindyNoonan\", \"Cindy Noonan\", tweet)\n",
        "    tweet = re.sub(r\"RT_America\", \"RT America\", tweet)\n",
        "    tweet = re.sub(r\"narendramodi\", \"Narendra Modi\", tweet)\n",
        "    tweet = re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", tweet)\n",
        "    tweet = re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", tweet)\n",
        "    tweet = re.sub(r\"alexbelloli\", \"Alex Belloli\", tweet)\n",
        "    tweet = re.sub(r\"itsjustinstuart\", \"Justin Stuart\", tweet)\n",
        "    tweet = re.sub(r\"gunsense\", \"gun sense\", tweet)\n",
        "    tweet = re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", tweet)\n",
        "    tweet = re.sub(r\"RoyalCarribean\", \"Royal Carribean\", tweet)\n",
        "    tweet = re.sub(r\"samanthaturne19\", \"Samantha Turner\", tweet)\n",
        "    tweet = re.sub(r\"JonVoyage\", \"Jon Stewart\", tweet)\n",
        "    tweet = re.sub(r\"renew911health\", \"renew 911 health\", tweet)\n",
        "    tweet = re.sub(r\"SuryaRay\", \"Surya Ray\", tweet)\n",
        "    tweet = re.sub(r\"pattonoswalt\", \"Patton Oswalt\", tweet)\n",
        "    tweet = re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", tweet)\n",
        "    tweet = re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", tweet)\n",
        "    tweet = re.sub(r\"pmarca\", \"Marc Andreessen\", tweet)\n",
        "    tweet = re.sub(r\"pdx911\", \"Portland Police\", tweet)\n",
        "    tweet = re.sub(r\"jamaicaplain\", \"Jamaica Plain\", tweet)\n",
        "    tweet = re.sub(r\"Japton\", \"Arkansas\", tweet)\n",
        "    tweet = re.sub(r\"RouteComplex\", \"Route Complex\", tweet)\n",
        "    tweet = re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", tweet)\n",
        "    tweet = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", tweet)\n",
        "    tweet = re.sub(r\"Politifiact\", \"PolitiFact\", tweet)\n",
        "    tweet = re.sub(r\"Hiroshima70\", \"Hiroshima\", tweet)\n",
        "    tweet = re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", tweet)\n",
        "    tweet = re.sub(r\"versethe\", \"verse the\", tweet)\n",
        "    tweet = re.sub(r\"TubeStrike\", \"Tube Strike\", tweet)\n",
        "    tweet = re.sub(r\"MissionHills\", \"Mission Hills\", tweet)\n",
        "    tweet = re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", tweet)\n",
        "    tweet = re.sub(r\"NANKANA\", \"Nankana\", tweet)\n",
        "    tweet = re.sub(r\"SAHIB\", \"Sahib\", tweet)\n",
        "    tweet = re.sub(r\"PAKPATTAN\", \"Pakpattan\", tweet)\n",
        "    tweet = re.sub(r\"Newz_Sacramento\", \"News Sacramento\", tweet)\n",
        "    tweet = re.sub(r\"gofundme\", \"go fund me\", tweet)\n",
        "    tweet = re.sub(r\"pmharper\", \"Stephen Harper\", tweet)\n",
        "    tweet = re.sub(r\"IvanBerroa\", \"Ivan Berroa\", tweet)\n",
        "    tweet = re.sub(r\"LosDelSonido\", \"Los Del Sonido\", tweet)\n",
        "    tweet = re.sub(r\"bancodeseries\", \"banco de series\", tweet)\n",
        "    tweet = re.sub(r\"timkaine\", \"Tim Kaine\", tweet)\n",
        "    tweet = re.sub(r\"IdentityTheft\", \"Identity Theft\", tweet)\n",
        "    tweet = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", tweet)\n",
        "    tweet = re.sub(r\"mishacollins\", \"Misha Collins\", tweet)\n",
        "    tweet = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", tweet)\n",
        "    tweet = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", tweet)\n",
        "    tweet = re.sub(r\"Kowing\", \"Knowing\", tweet)\n",
        "    tweet = re.sub(r\"ScreamQueens\", \"Scream Queens\", tweet)\n",
        "    tweet = re.sub(r\"AskCharley\", \"Ask Charley\", tweet)\n",
        "    tweet = re.sub(r\"BlizzHeroes\", \"Heroes of the Storm\", tweet)\n",
        "    tweet = re.sub(r\"BradleyBrad47\", \"Bradley Brad\", tweet)\n",
        "    tweet = re.sub(r\"HannaPH\", \"Typhoon Hanna\", tweet)\n",
        "    tweet = re.sub(r\"meinlcymbals\", \"MEINL Cymbals\", tweet)\n",
        "    tweet = re.sub(r\"Ptbo\", \"Peterborough\", tweet)\n",
        "    tweet = re.sub(r\"cnnbrk\", \"CNN Breaking News\", tweet)\n",
        "    tweet = re.sub(r\"IndianNews\", \"Indian News\", tweet)\n",
        "    tweet = re.sub(r\"savebees\", \"save bees\", tweet)\n",
        "    tweet = re.sub(r\"GreenHarvard\", \"Green Harvard\", tweet)\n",
        "    tweet = re.sub(r\"StandwithPP\", \"Stand with planned parenthood\", tweet)\n",
        "    tweet = re.sub(r\"hermancranston\", \"Herman Cranston\", tweet)\n",
        "    tweet = re.sub(r\"WMUR9\", \"WMUR-TV\", tweet)\n",
        "    tweet = re.sub(r\"RockBottomRadFM\", \"Rock Bottom Radio\", tweet)\n",
        "    tweet = re.sub(r\"ameenshaikh3\", \"Ameen Shaikh\", tweet)\n",
        "    tweet = re.sub(r\"ProSyn\", \"Project Syndicate\", tweet)\n",
        "    tweet = re.sub(r\"Daesh\", \"ISIS\", tweet)\n",
        "    tweet = re.sub(r\"s2g\", \"swear to god\", tweet)\n",
        "    tweet = re.sub(r\"listenlive\", \"listen live\", tweet)\n",
        "    tweet = re.sub(r\"CDCgov\", \"Centers for Disease Control and Prevention\", tweet)\n",
        "    tweet = re.sub(r\"FoxNew\", \"Fox News\", tweet)\n",
        "    tweet = re.sub(r\"CBSBigBrother\", \"Big Brother\", tweet)\n",
        "    tweet = re.sub(r\"JulieDiCaro\", \"Julie DiCaro\", tweet)\n",
        "    tweet = re.sub(r\"theadvocatemag\", \"The Advocate Magazine\", tweet)\n",
        "    tweet = re.sub(r\"RohnertParkDPS\", \"Rohnert Park Police Department\", tweet)\n",
        "    tweet = re.sub(r\"THISIZBWRIGHT\", \"Bonnie Wright\", tweet)\n",
        "    tweet = re.sub(r\"Popularmmos\", \"Popular MMOs\", tweet)\n",
        "    tweet = re.sub(r\"WildHorses\", \"Wild Horses\", tweet)\n",
        "    tweet = re.sub(r\"FantasticFour\", \"Fantastic Four\", tweet)\n",
        "    tweet = re.sub(r\"HORNDALE\", \"Horndale\", tweet)\n",
        "    tweet = re.sub(r\"PINER\", \"Piner\", tweet)\n",
        "    tweet = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", tweet)\n",
        "    tweet = re.sub(r\"thatswhatfriendsarefor\", \"that is what friends are for\", tweet)\n",
        "    tweet = re.sub(r\"residualincome\", \"residual income\", tweet)\n",
        "    tweet = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", tweet)\n",
        "    tweet = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", tweet)\n",
        "    tweet = re.sub(r\"AmazonDeals\", \"Amazon Deals\", tweet)\n",
        "    tweet = re.sub(r\"MissCharleyWebb\", \"Charley Webb\", tweet)\n",
        "    tweet = re.sub(r\"shoalstraffic\", \"shoals traffic\", tweet)\n",
        "    tweet = re.sub(r\"GeorgeFoster72\", \"George Foster\", tweet)\n",
        "    tweet = re.sub(r\"pop2015\", \"pop 2015\", tweet)\n",
        "    tweet = re.sub(r\"_PokemonCards_\", \"Pokemon Cards\", tweet)\n",
        "    tweet = re.sub(r\"DianneG\", \"Dianne Gallagher\", tweet)\n",
        "    tweet = re.sub(r\"KashmirConflict\", \"Kashmir Conflict\", tweet)\n",
        "    tweet = re.sub(r\"BritishBakeOff\", \"British Bake Off\", tweet)\n",
        "    tweet = re.sub(r\"FreeKashmir\", \"Free Kashmir\", tweet)\n",
        "    tweet = re.sub(r\"mattmosley\", \"Matt Mosley\", tweet)\n",
        "    tweet = re.sub(r\"BishopFred\", \"Bishop Fred\", tweet)\n",
        "    tweet = re.sub(r\"EndConflict\", \"End Conflict\", tweet)\n",
        "    tweet = re.sub(r\"EndOccupation\", \"End Occupation\", tweet)\n",
        "    tweet = re.sub(r\"UNHEALED\", \"unhealed\", tweet)\n",
        "    tweet = re.sub(r\"CharlesDagnall\", \"Charles Dagnall\", tweet)\n",
        "    tweet = re.sub(r\"Latestnews\", \"Latest news\", tweet)\n",
        "    tweet = re.sub(r\"KindleCountdown\", \"Kindle Countdown\", tweet)\n",
        "    tweet = re.sub(r\"NoMoreHandouts\", \"No More Handouts\", tweet)\n",
        "    tweet = re.sub(r\"datingtips\", \"dating tips\", tweet)\n",
        "    tweet = re.sub(r\"charlesadler\", \"Charles Adler\", tweet)\n",
        "    tweet = re.sub(r\"twia\", \"Texas Windstorm Insurance Association\", tweet)\n",
        "    tweet = re.sub(r\"txlege\", \"Texas Legislature\", tweet)\n",
        "    tweet = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", tweet)\n",
        "    tweet = re.sub(r\"Newss\", \"News\", tweet)\n",
        "    tweet = re.sub(r\"hempoil\", \"hemp oil\", tweet)\n",
        "    tweet = re.sub(r\"CommoditiesAre\", \"Commodities are\", tweet)\n",
        "    tweet = re.sub(r\"tubestrike\", \"tube strike\", tweet)\n",
        "    tweet = re.sub(r\"JoeNBC\", \"Joe Scarborough\", tweet)\n",
        "    tweet = re.sub(r\"LiteraryCakes\", \"Literary Cakes\", tweet)\n",
        "    tweet = re.sub(r\"TI5\", \"The International 5\", tweet)\n",
        "    tweet = re.sub(r\"thehill\", \"the hill\", tweet)\n",
        "    tweet = re.sub(r\"3others\", \"3 others\", tweet)\n",
        "    tweet = re.sub(r\"stighefootball\", \"Sam Tighe\", tweet)\n",
        "    tweet = re.sub(r\"whatstheimportantvideo\", \"what is the important video\", tweet)\n",
        "    tweet = re.sub(r\"ClaudioMeloni\", \"Claudio Meloni\", tweet)\n",
        "    tweet = re.sub(r\"DukeSkywalker\", \"Duke Skywalker\", tweet)\n",
        "    tweet = re.sub(r\"carsonmwr\", \"Fort Carson\", tweet)\n",
        "    tweet = re.sub(r\"offdishduty\", \"off dish duty\", tweet)\n",
        "    tweet = re.sub(r\"andword\", \"and word\", tweet)\n",
        "    tweet = re.sub(r\"rhodeisland\", \"Rhode Island\", tweet)\n",
        "    tweet = re.sub(r\"easternoregon\", \"Eastern Oregon\", tweet)\n",
        "    tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\n",
        "    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n",
        "    tweet = re.sub(r\"57am\", \"57 am\", tweet)\n",
        "    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n",
        "    tweet = re.sub(r\"JacobHoggard\", \"Jacob Hoggard\", tweet)\n",
        "    tweet = re.sub(r\"newnewnew\", \"new new new\", tweet)\n",
        "    tweet = re.sub(r\"under50\", \"under 50\", tweet)\n",
        "    tweet = re.sub(r\"getitbeforeitsgone\", \"get it before it is gone\", tweet)\n",
        "    tweet = re.sub(r\"freshoutofthebox\", \"fresh out of the box\", tweet)\n",
        "    tweet = re.sub(r\"amwriting\", \"am writing\", tweet)\n",
        "    tweet = re.sub(r\"Bokoharm\", \"Boko Haram\", tweet)\n",
        "    tweet = re.sub(r\"Nowlike\", \"Now like\", tweet)\n",
        "    tweet = re.sub(r\"seasonfrom\", \"season from\", tweet)\n",
        "    tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\n",
        "    tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n",
        "    tweet = re.sub(r\"sicklife\", \"sick life\", tweet)\n",
        "    tweet = re.sub(r\"yycweather\", \"Calgary Weather\", tweet)\n",
        "    tweet = re.sub(r\"calgarysun\", \"Calgary Sun\", tweet)\n",
        "    tweet = re.sub(r\"approachng\", \"approaching\", tweet)\n",
        "    tweet = re.sub(r\"evng\", \"evening\", tweet)\n",
        "    tweet = re.sub(r\"Sumthng\", \"something\", tweet)\n",
        "    tweet = re.sub(r\"EllenPompeo\", \"Ellen Pompeo\", tweet)\n",
        "    tweet = re.sub(r\"shondarhimes\", \"Shonda Rhimes\", tweet)\n",
        "    tweet = re.sub(r\"ABCNetwork\", \"ABC Network\", tweet)\n",
        "    tweet = re.sub(r\"SushmaSwaraj\", \"Sushma Swaraj\", tweet)\n",
        "    tweet = re.sub(r\"pray4japan\", \"Pray for Japan\", tweet)\n",
        "    tweet = re.sub(r\"hope4japan\", \"Hope for Japan\", tweet)\n",
        "    tweet = re.sub(r\"Illusionimagess\", \"Illusion images\", tweet)\n",
        "    tweet = re.sub(r\"SummerUnderTheStars\", \"Summer Under The Stars\", tweet)\n",
        "    tweet = re.sub(r\"ShallWeDance\", \"Shall We Dance\", tweet)\n",
        "    tweet = re.sub(r\"TCMParty\", \"TCM Party\", tweet)\n",
        "    tweet = re.sub(r\"marijuananews\", \"marijuana news\", tweet)\n",
        "    tweet = re.sub(r\"onbeingwithKristaTippett\", \"on being with Krista Tippett\", tweet)\n",
        "    tweet = re.sub(r\"Beingtweets\", \"Being tweets\", tweet)\n",
        "    tweet = re.sub(r\"newauthors\", \"new authors\", tweet)\n",
        "    tweet = re.sub(r\"remedyyyy\", \"remedy\", tweet)\n",
        "    tweet = re.sub(r\"44PM\", \"44 PM\", tweet)\n",
        "    tweet = re.sub(r\"HeadlinesApp\", \"Headlines App\", tweet)\n",
        "    tweet = re.sub(r\"40PM\", \"40 PM\", tweet)\n",
        "    tweet = re.sub(r\"myswc\", \"Severe Weather Center\", tweet)\n",
        "    tweet = re.sub(r\"ithats\", \"that is\", tweet)\n",
        "    tweet = re.sub(r\"icouldsitinthismomentforever\", \"I could sit in this moment forever\", tweet)\n",
        "    tweet = re.sub(r\"FatLoss\", \"Fat Loss\", tweet)\n",
        "    tweet = re.sub(r\"02PM\", \"02 PM\", tweet)\n",
        "    tweet = re.sub(r\"MetroFmTalk\", \"Metro Fm Talk\", tweet)\n",
        "    tweet = re.sub(r\"Bstrd\", \"bastard\", tweet)\n",
        "    tweet = re.sub(r\"bldy\", \"bloody\", tweet)\n",
        "    tweet = re.sub(r\"MetrofmTalk\", \"Metro Fm Talk\", tweet)\n",
        "    tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\n",
        "    tweet = re.sub(r\"BBCNewsAsia\", \"BBC News Asia\", tweet)\n",
        "    tweet = re.sub(r\"BehindTheScenes\", \"Behind The Scenes\", tweet)\n",
        "    tweet = re.sub(r\"GeorgeTakei\", \"George Takei\", tweet)\n",
        "    tweet = re.sub(r\"WomensWeeklyMag\", \"Womens Weekly Magazine\", tweet)\n",
        "    tweet = re.sub(r\"SurvivorsGuidetoEarth\", \"Survivors Guide to Earth\", tweet)\n",
        "    tweet = re.sub(r\"incubusband\", \"incubus band\", tweet)\n",
        "    tweet = re.sub(r\"Babypicturethis\", \"Baby picture this\", tweet)\n",
        "    tweet = re.sub(r\"BombEffects\", \"Bomb Effects\", tweet)\n",
        "    tweet = re.sub(r\"win10\", \"Windows 10\", tweet)\n",
        "    tweet = re.sub(r\"idkidk\", \"I do not know I do not know\", tweet)\n",
        "    tweet = re.sub(r\"TheWalkingDead\", \"The Walking Dead\", tweet)\n",
        "    tweet = re.sub(r\"amyschumer\", \"Amy Schumer\", tweet)\n",
        "    tweet = re.sub(r\"crewlist\", \"crew list\", tweet)\n",
        "    tweet = re.sub(r\"Erdogans\", \"Erdogan\", tweet)\n",
        "    tweet = re.sub(r\"BBCLive\", \"BBC Live\", tweet)\n",
        "    tweet = re.sub(r\"TonyAbbottMHR\", \"Tony Abbott\", tweet)\n",
        "    tweet = re.sub(r\"paulmyerscough\", \"Paul Myerscough\", tweet)\n",
        "    tweet = re.sub(r\"georgegallagher\", \"George Gallagher\", tweet)\n",
        "    tweet = re.sub(r\"JimmieJohnson\", \"Jimmie Johnson\", tweet)\n",
        "    tweet = re.sub(r\"pctool\", \"pc tool\", tweet)\n",
        "    tweet = re.sub(r\"DoingHashtagsRight\", \"Doing Hashtags Right\", tweet)\n",
        "    tweet = re.sub(r\"ThrowbackThursday\", \"Throwback Thursday\", tweet)\n",
        "    tweet = re.sub(r\"SnowBackSunday\", \"Snowback Sunday\", tweet)\n",
        "    tweet = re.sub(r\"LakeEffect\", \"Lake Effect\", tweet)\n",
        "    tweet = re.sub(r\"RTphotographyUK\", \"Richard Thomas Photography UK\", tweet)\n",
        "    tweet = re.sub(r\"BigBang_CBS\", \"Big Bang CBS\", tweet)\n",
        "    tweet = re.sub(r\"writerslife\", \"writers life\", tweet)\n",
        "    tweet = re.sub(r\"NaturalBirth\", \"Natural Birth\", tweet)\n",
        "    tweet = re.sub(r\"UnusualWords\", \"Unusual Words\", tweet)\n",
        "    tweet = re.sub(r\"wizkhalifa\", \"Wiz Khalifa\", tweet)\n",
        "    tweet = re.sub(r\"acreativedc\", \"a creative DC\", tweet)\n",
        "    tweet = re.sub(r\"vscodc\", \"vsco DC\", tweet)\n",
        "    tweet = re.sub(r\"VSCOcam\", \"vsco camera\", tweet)\n",
        "    tweet = re.sub(r\"TheBEACHDC\", \"The beach DC\", tweet)\n",
        "    tweet = re.sub(r\"buildingmuseum\", \"building museum\", tweet)\n",
        "    tweet = re.sub(r\"WorldOil\", \"World Oil\", tweet)\n",
        "    tweet = re.sub(r\"redwedding\", \"red wedding\", tweet)\n",
        "    tweet = re.sub(r\"AmazingRaceCanada\", \"Amazing Race Canada\", tweet)\n",
        "    tweet = re.sub(r\"WakeUpAmerica\", \"Wake Up America\", tweet)\n",
        "    tweet = re.sub(r\"\\\\Allahuakbar\\\\\", \"Allahu Akbar\", tweet)\n",
        "    tweet = re.sub(r\"bleased\", \"blessed\", tweet)\n",
        "    tweet = re.sub(r\"nigeriantribune\", \"Nigerian Tribune\", tweet)\n",
        "    tweet = re.sub(r\"HIDEO_KOJIMA_EN\", \"Hideo Kojima\", tweet)\n",
        "    tweet = re.sub(r\"FusionFestival\", \"Fusion Festival\", tweet)\n",
        "    tweet = re.sub(r\"50Mixed\", \"50 Mixed\", tweet)\n",
        "    tweet = re.sub(r\"NoAgenda\", \"No Agenda\", tweet)\n",
        "    tweet = re.sub(r\"WhiteGenocide\", \"White Genocide\", tweet)\n",
        "    tweet = re.sub(r\"dirtylying\", \"dirty lying\", tweet)\n",
        "    tweet = re.sub(r\"SyrianRefugees\", \"Syrian Refugees\", tweet)\n",
        "    tweet = re.sub(r\"changetheworld\", \"change the world\", tweet)\n",
        "    tweet = re.sub(r\"Ebolacase\", \"Ebola case\", tweet)\n",
        "    tweet = re.sub(r\"mcgtech\", \"mcg technologies\", tweet)\n",
        "    tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\n",
        "    tweet = re.sub(r\"advancedwarfare\", \"advanced warfare\", tweet)\n",
        "    tweet = re.sub(r\"letsFootball\", \"let us Football\", tweet)\n",
        "    tweet = re.sub(r\"LateNiteMix\", \"late night mix\", tweet)\n",
        "    tweet = re.sub(r\"PhilCollinsFeed\", \"Phil Collins\", tweet)\n",
        "    tweet = re.sub(r\"RudyHavenstein\", \"Rudy Havenstein\", tweet)\n",
        "    tweet = re.sub(r\"22PM\", \"22 PM\", tweet)\n",
        "    tweet = re.sub(r\"54am\", \"54 AM\", tweet)\n",
        "    tweet = re.sub(r\"38am\", \"38 AM\", tweet)\n",
        "    tweet = re.sub(r\"OldFolkExplainStuff\", \"Old Folk Explain Stuff\", tweet)\n",
        "    tweet = re.sub(r\"BlacklivesMatter\", \"Black Lives Matter\", tweet)\n",
        "    tweet = re.sub(r\"InsaneLimits\", \"Insane Limits\", tweet)\n",
        "    tweet = re.sub(r\"youcantsitwithus\", \"you cannot sit with us\", tweet)\n",
        "    tweet = re.sub(r\"2k15\", \"2015\", tweet)\n",
        "    tweet = re.sub(r\"TheIran\", \"Iran\", tweet)\n",
        "    tweet = re.sub(r\"JimmyFallon\", \"Jimmy Fallon\", tweet)\n",
        "    tweet = re.sub(r\"AlbertBrooks\", \"Albert Brooks\", tweet)\n",
        "    tweet = re.sub(r\"defense_news\", \"defense news\", tweet)\n",
        "    tweet = re.sub(r\"nuclearrcSA\", \"Nuclear Risk Control Self Assessment\", tweet)\n",
        "    tweet = re.sub(r\"Auspol\", \"Australia Politics\", tweet)\n",
        "    tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\n",
        "    tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n",
        "    tweet = re.sub(r\"truthfrequencyradio\", \"Truth Frequency Radio\", tweet)\n",
        "    tweet = re.sub(r\"ErasureIsNotEquality\", \"Erasure is not equality\", tweet)\n",
        "    tweet = re.sub(r\"ProBonoNews\", \"Pro Bono News\", tweet)\n",
        "    tweet = re.sub(r\"JakartaPost\", \"Jakarta Post\", tweet)\n",
        "    tweet = re.sub(r\"toopainful\", \"too painful\", tweet)\n",
        "    tweet = re.sub(r\"melindahaunton\", \"Melinda Haunton\", tweet)\n",
        "    tweet = re.sub(r\"NoNukes\", \"No Nukes\", tweet)\n",
        "    tweet = re.sub(r\"curryspcworld\", \"Currys PC World\", tweet)\n",
        "    tweet = re.sub(r\"ineedcake\", \"I need cake\", tweet)\n",
        "    tweet = re.sub(r\"blackforestgateau\", \"black forest gateau\", tweet)\n",
        "    tweet = re.sub(r\"BBCOne\", \"BBC One\", tweet)\n",
        "    tweet = re.sub(r\"AlexxPage\", \"Alex Page\", tweet)\n",
        "    tweet = re.sub(r\"jonathanserrie\", \"Jonathan Serrie\", tweet)\n",
        "    tweet = re.sub(r\"SocialJerkBlog\", \"Social Jerk Blog\", tweet)\n",
        "    tweet = re.sub(r\"ChelseaVPeretti\", \"Chelsea Peretti\", tweet)\n",
        "    tweet = re.sub(r\"irongiant\", \"iron giant\", tweet)\n",
        "    tweet = re.sub(r\"RonFunches\", \"Ron Funches\", tweet)\n",
        "    tweet = re.sub(r\"TimCook\", \"Tim Cook\", tweet)\n",
        "    tweet = re.sub(r\"sebastianstanisaliveandwell\", \"Sebastian Stan is alive and well\", tweet)\n",
        "    tweet = re.sub(r\"Madsummer\", \"Mad summer\", tweet)\n",
        "    tweet = re.sub(r\"NowYouKnow\", \"Now you know\", tweet)\n",
        "    tweet = re.sub(r\"concertphotography\", \"concert photography\", tweet)\n",
        "    tweet = re.sub(r\"TomLandry\", \"Tom Landry\", tweet)\n",
        "    tweet = re.sub(r\"showgirldayoff\", \"show girl day off\", tweet)\n",
        "    tweet = re.sub(r\"Yougslavia\", \"Yugoslavia\", tweet)\n",
        "    tweet = re.sub(r\"QuantumDataInformatics\", \"Quantum Data Informatics\", tweet)\n",
        "    tweet = re.sub(r\"FromTheDesk\", \"From The Desk\", tweet)\n",
        "    tweet = re.sub(r\"TheaterTrial\", \"Theater Trial\", tweet)\n",
        "    tweet = re.sub(r\"CatoInstitute\", \"Cato Institute\", tweet)\n",
        "    tweet = re.sub(r\"EmekaGift\", \"Emeka Gift\", tweet)\n",
        "    tweet = re.sub(r\"LetsBe_Rational\", \"Let us be rational\", tweet)\n",
        "    tweet = re.sub(r\"Cynicalreality\", \"Cynical reality\", tweet)\n",
        "    tweet = re.sub(r\"FredOlsenCruise\", \"Fred Olsen Cruise\", tweet)\n",
        "    tweet = re.sub(r\"NotSorry\", \"not sorry\", tweet)\n",
        "    tweet = re.sub(r\"UseYourWords\", \"use your words\", tweet)\n",
        "    tweet = re.sub(r\"WordoftheDay\", \"word of the day\", tweet)\n",
        "    tweet = re.sub(r\"Dictionarycom\", \"Dictionary.com\", tweet)\n",
        "    tweet = re.sub(r\"TheBrooklynLife\", \"The Brooklyn Life\", tweet)\n",
        "    tweet = re.sub(r\"jokethey\", \"joke they\", tweet)\n",
        "    tweet = re.sub(r\"nflweek1picks\", \"NFL week 1 picks\", tweet)\n",
        "    tweet = re.sub(r\"uiseful\", \"useful\", tweet)\n",
        "    tweet = re.sub(r\"JusticeDotOrg\", \"The American Association for Justice\", tweet)\n",
        "    tweet = re.sub(r\"autoaccidents\", \"auto accidents\", tweet)\n",
        "    tweet = re.sub(r\"SteveGursten\", \"Steve Gursten\", tweet)\n",
        "    tweet = re.sub(r\"MichiganAutoLaw\", \"Michigan Auto Law\", tweet)\n",
        "    tweet = re.sub(r\"birdgang\", \"bird gang\", tweet)\n",
        "    tweet = re.sub(r\"nflnetwork\", \"NFL Network\", tweet)\n",
        "    tweet = re.sub(r\"NYDNSports\", \"NY Daily News Sports\", tweet)\n",
        "    tweet = re.sub(r\"RVacchianoNYDN\", \"Ralph Vacchiano NY Daily News\", tweet)\n",
        "    tweet = re.sub(r\"EdmontonEsks\", \"Edmonton Eskimos\", tweet)\n",
        "    tweet = re.sub(r\"david_brelsford\", \"David Brelsford\", tweet)\n",
        "    tweet = re.sub(r\"TOI_India\", \"The Times of India\", tweet)\n",
        "    tweet = re.sub(r\"hegot\", \"he got\", tweet)\n",
        "    tweet = re.sub(r\"SkinsOn9\", \"Skins on 9\", tweet)\n",
        "    tweet = re.sub(r\"sothathappened\", \"so that happened\", tweet)\n",
        "    tweet = re.sub(r\"LCOutOfDoors\", \"LC Out Of Doors\", tweet)\n",
        "    tweet = re.sub(r\"NationFirst\", \"Nation First\", tweet)\n",
        "    tweet = re.sub(r\"IndiaToday\", \"India Today\", tweet)\n",
        "    tweet = re.sub(r\"HLPS\", \"helps\", tweet)\n",
        "    tweet = re.sub(r\"HOSTAGESTHROSW\", \"hostages throw\", tweet)\n",
        "    tweet = re.sub(r\"SNCTIONS\", \"sanctions\", tweet)\n",
        "    tweet = re.sub(r\"BidTime\", \"Bid Time\", tweet)\n",
        "    tweet = re.sub(r\"crunchysensible\", \"crunchy sensible\", tweet)\n",
        "    tweet = re.sub(r\"RandomActsOfRomance\", \"Random acts of romance\", tweet)\n",
        "    tweet = re.sub(r\"MomentsAtHill\", \"Moments at hill\", tweet)\n",
        "    tweet = re.sub(r\"eatshit\", \"eat shit\", tweet)\n",
        "    tweet = re.sub(r\"liveleakfun\", \"live leak fun\", tweet)\n",
        "    tweet = re.sub(r\"SahelNews\", \"Sahel News\", tweet)\n",
        "    tweet = re.sub(r\"abc7newsbayarea\", \"ABC 7 News Bay Area\", tweet)\n",
        "    tweet = re.sub(r\"facilitiesmanagement\", \"facilities management\", tweet)\n",
        "    tweet = re.sub(r\"facilitydude\", \"facility dude\", tweet)\n",
        "    tweet = re.sub(r\"CampLogistics\", \"Camp logistics\", tweet)\n",
        "    tweet = re.sub(r\"alaskapublic\", \"Alaska public\", tweet)\n",
        "    tweet = re.sub(r\"MarketResearch\", \"Market Research\", tweet)\n",
        "    tweet = re.sub(r\"AccuracyEsports\", \"Accuracy Esports\", tweet)\n",
        "    tweet = re.sub(r\"TheBodyShopAust\", \"The Body Shop Australia\", tweet)\n",
        "    tweet = re.sub(r\"yychail\", \"Calgary hail\", tweet)\n",
        "    tweet = re.sub(r\"yyctraffic\", \"Calgary traffic\", tweet)\n",
        "    tweet = re.sub(r\"eliotschool\", \"eliot school\", tweet)\n",
        "    tweet = re.sub(r\"TheBrokenCity\", \"The Broken City\", tweet)\n",
        "    tweet = re.sub(r\"OldsFireDept\", \"Olds Fire Department\", tweet)\n",
        "    tweet = re.sub(r\"RiverComplex\", \"River Complex\", tweet)\n",
        "    tweet = re.sub(r\"fieldworksmells\", \"field work smells\", tweet)\n",
        "    tweet = re.sub(r\"IranElection\", \"Iran Election\", tweet)\n",
        "    tweet = re.sub(r\"glowng\", \"glowing\", tweet)\n",
        "    tweet = re.sub(r\"kindlng\", \"kindling\", tweet)\n",
        "    tweet = re.sub(r\"riggd\", \"rigged\", tweet)\n",
        "    tweet = re.sub(r\"slownewsday\", \"slow news day\", tweet)\n",
        "    tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n",
        "    tweet = re.sub(r\"abc7chicago\", \"ABC 7 Chicago\", tweet)\n",
        "    tweet = re.sub(r\"copolitics\", \"Colorado Politics\", tweet)\n",
        "    tweet = re.sub(r\"AdilGhumro\", \"Adil Ghumro\", tweet)\n",
        "    tweet = re.sub(r\"netbots\", \"net bots\", tweet)\n",
        "    tweet = re.sub(r\"byebyeroad\", \"bye bye road\", tweet)\n",
        "    tweet = re.sub(r\"massiveflooding\", \"massive flooding\", tweet)\n",
        "    tweet = re.sub(r\"EndofUS\", \"End of United States\", tweet)\n",
        "    tweet = re.sub(r\"35PM\", \"35 PM\", tweet)\n",
        "    tweet = re.sub(r\"greektheatrela\", \"Greek Theatre Los Angeles\", tweet)\n",
        "    tweet = re.sub(r\"76mins\", \"76 minutes\", tweet)\n",
        "    tweet = re.sub(r\"publicsafetyfirst\", \"public safety first\", tweet)\n",
        "    tweet = re.sub(r\"livesmatter\", \"lives matter\", tweet)\n",
        "    tweet = re.sub(r\"myhometown\", \"my hometown\", tweet)\n",
        "    tweet = re.sub(r\"tankerfire\", \"tanker fire\", tweet)\n",
        "    tweet = re.sub(r\"MEMORIALDAY\", \"memorial day\", tweet)\n",
        "    tweet = re.sub(r\"MEMORIAL_DAY\", \"memorial day\", tweet)\n",
        "    tweet = re.sub(r\"instaxbooty\", \"instagram booty\", tweet)\n",
        "    tweet = re.sub(r\"Jerusalem_Post\", \"Jerusalem Post\", tweet)\n",
        "    tweet = re.sub(r\"WayneRooney_INA\", \"Wayne Rooney\", tweet)\n",
        "    tweet = re.sub(r\"VirtualReality\", \"Virtual Reality\", tweet)\n",
        "    tweet = re.sub(r\"OculusRift\", \"Oculus Rift\", tweet)\n",
        "    tweet = re.sub(r\"OwenJones84\", \"Owen Jones\", tweet)\n",
        "    tweet = re.sub(r\"jeremycorbyn\", \"Jeremy Corbyn\", tweet)\n",
        "    tweet = re.sub(r\"paulrogers002\", \"Paul Rogers\", tweet)\n",
        "    tweet = re.sub(r\"mortalkombatx\", \"Mortal Kombat X\", tweet)\n",
        "    tweet = re.sub(r\"mortalkombat\", \"Mortal Kombat\", tweet)\n",
        "    tweet = re.sub(r\"FilipeCoelho92\", \"Filipe Coelho\", tweet)\n",
        "    tweet = re.sub(r\"OnlyQuakeNews\", \"Only Quake News\", tweet)\n",
        "    tweet = re.sub(r\"kostumes\", \"costumes\", tweet)\n",
        "    tweet = re.sub(r\"YEEESSSS\", \"yes\", tweet)\n",
        "    tweet = re.sub(r\"ToshikazuKatayama\", \"Toshikazu Katayama\", tweet)\n",
        "    tweet = re.sub(r\"IntlDevelopment\", \"Intl Development\", tweet)\n",
        "    tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n",
        "    tweet = re.sub(r\"WereNotGruberVoters\", \"We are not gruber voters\", tweet)\n",
        "    tweet = re.sub(r\"NewsThousands\", \"News Thousands\", tweet)\n",
        "    tweet = re.sub(r\"EdmundAdamus\", \"Edmund Adamus\", tweet)\n",
        "    tweet = re.sub(r\"EyewitnessWV\", \"Eye witness WV\", tweet)\n",
        "    tweet = re.sub(r\"PhiladelphiaMuseu\", \"Philadelphia Museum\", tweet)\n",
        "    tweet = re.sub(r\"DublinComicCon\", \"Dublin Comic Con\", tweet)\n",
        "    tweet = re.sub(r\"NicholasBrendon\", \"Nicholas Brendon\", tweet)\n",
        "    tweet = re.sub(r\"Alltheway80s\", \"All the way 80s\", tweet)\n",
        "    tweet = re.sub(r\"FromTheField\", \"From the field\", tweet)\n",
        "    tweet = re.sub(r\"NorthIowa\", \"North Iowa\", tweet)\n",
        "    tweet = re.sub(r\"WillowFire\", \"Willow Fire\", tweet)\n",
        "    tweet = re.sub(r\"MadRiverComplex\", \"Mad River Complex\", tweet)\n",
        "    tweet = re.sub(r\"feelingmanly\", \"feeling manly\", tweet)\n",
        "    tweet = re.sub(r\"stillnotoverit\", \"still not over it\", tweet)\n",
        "    tweet = re.sub(r\"FortitudeValley\", \"Fortitude Valley\", tweet)\n",
        "    tweet = re.sub(r\"CoastpowerlineTramTr\", \"Coast powerline\", tweet)\n",
        "    tweet = re.sub(r\"ServicesGold\", \"Services Gold\", tweet)\n",
        "    tweet = re.sub(r\"NewsbrokenEmergency\", \"News broken emergency\", tweet)\n",
        "    tweet = re.sub(r\"Evaucation\", \"evacuation\", tweet)\n",
        "    tweet = re.sub(r\"leaveevacuateexitbe\", \"leave evacuate exit be\", tweet)\n",
        "    tweet = re.sub(r\"P_EOPLE\", \"PEOPLE\", tweet)\n",
        "    tweet = re.sub(r\"Tubestrike\", \"tube strike\", tweet)\n",
        "    tweet = re.sub(r\"CLASS_SICK\", \"CLASS SICK\", tweet)\n",
        "    tweet = re.sub(r\"localplumber\", \"local plumber\", tweet)\n",
        "    tweet = re.sub(r\"awesomejobsiri\", \"awesome job siri\", tweet)\n",
        "    tweet = re.sub(r\"PayForItHow\", \"Pay for it how\", tweet)\n",
        "    tweet = re.sub(r\"ThisIsAfrica\", \"This is Africa\", tweet)\n",
        "    tweet = re.sub(r\"crimeairnetwork\", \"crime air network\", tweet)\n",
        "    tweet = re.sub(r\"KimAcheson\", \"Kim Acheson\", tweet)\n",
        "    tweet = re.sub(r\"cityofcalgary\", \"City of Calgary\", tweet)\n",
        "    tweet = re.sub(r\"prosyndicate\", \"pro syndicate\", tweet)\n",
        "    tweet = re.sub(r\"660NEWS\", \"660 NEWS\", tweet)\n",
        "    tweet = re.sub(r\"BusInsMagazine\", \"Business Insurance Magazine\", tweet)\n",
        "    tweet = re.sub(r\"wfocus\", \"focus\", tweet)\n",
        "    tweet = re.sub(r\"ShastaDam\", \"Shasta Dam\", tweet)\n",
        "    tweet = re.sub(r\"go2MarkFranco\", \"Mark Franco\", tweet)\n",
        "    tweet = re.sub(r\"StephGHinojosa\", \"Steph Hinojosa\", tweet)\n",
        "    tweet = re.sub(r\"Nashgrier\", \"Nash Grier\", tweet)\n",
        "    tweet = re.sub(r\"NashNewVideo\", \"Nash new video\", tweet)\n",
        "    tweet = re.sub(r\"IWouldntGetElectedBecause\", \"I would not get elected because\", tweet)\n",
        "    tweet = re.sub(r\"SHGames\", \"Sledgehammer Games\", tweet)\n",
        "    tweet = re.sub(r\"bedhair\", \"bed hair\", tweet)\n",
        "    tweet = re.sub(r\"JoelHeyman\", \"Joel Heyman\", tweet)\n",
        "    tweet = re.sub(r\"viaYouTube\", \"via YouTube\", tweet)\n",
        "           \n",
        "    # Urls\n",
        "    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n",
        "        \n",
        "    # Words with punctuations and special characters\n",
        "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
        "    for p in punctuations:\n",
        "        tweet = tweet.replace(p, f' {p} ')\n",
        "        \n",
        "    # ... and ..\n",
        "    tweet = tweet.replace('...', ' ... ')\n",
        "    if '...' not in tweet:\n",
        "        tweet = tweet.replace('..', ' ... ')      \n",
        "        \n",
        "    # Acronyms\n",
        "    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n",
        "    tweet = re.sub(r\"mÌ¼sica\", \"music\", tweet)\n",
        "    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n",
        "    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \n",
        "    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n",
        "    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n",
        "    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n",
        "    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n",
        "    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n",
        "    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n",
        "    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \n",
        "    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n",
        "    tweet = re.sub(r\"Suruc\", \"Sanliurfa\", tweet)   \n",
        "    \n",
        "    # Grouping same words without embeddings\n",
        "    tweet = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", tweet)\n",
        "    tweet = re.sub(r\"SOUDELOR\", \"Soudelor\", tweet)\n",
        "    \n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 79.1 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGNi2bU0tTcz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "29fbe0df-bdd7-406c-8a24-00edb43f7b17"
      },
      "source": [
        "def clean_text(text ):\n",
        "  # Si no es un string devuelvo un string vacio\n",
        "  if isinstance( text , str) == False: return ''\n",
        "  # text = super_manual_clean(text)\n",
        "\n",
        "  # Remuevo html entities b\n",
        "  text = html.unescape( text )\n",
        "\n",
        "  # # Hago decode de todos los textos que esten encodeados en formato url\n",
        "  text = urllib.parse.unquote( text )\n",
        "\n",
        "\n",
        "  # Intento remover todos los links que figuren en el texto\n",
        "  text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text )\n",
        "\n",
        "  # Remuevo todas las citas de usuario, si arranca con @ es un user quote\n",
        "  # text = \" \".join( [a for a in text.split() if a[0] != '@'] )\n",
        "\n",
        "  # Transformo citas de usuario y hashtag en solo la palabra\n",
        "  text.replace('@', ' ').replace('#', ' ').replace('-', ' ').replace('_' , ' ')\n",
        "\n",
        "  \n",
        "  # Hago camel case split\n",
        "  text = \" \".join(re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', text)).split())\n",
        "\n",
        "  # Remuevo todos los signos de puntuacion\n",
        "  # text = text.translate(str.maketrans('','',string.punctuation))\n",
        "\n",
        "  # Remuevo todos los numeros de la cadena de caracteres\n",
        "  # text = text.translate(str.maketrans('','','1234567890'))\n",
        "\n",
        "  # Tokenize: Split the sentence into words\n",
        "  word_list = nltk.word_tokenize(text)\n",
        "\n",
        "  # Hago lematización\n",
        "  # Lemmatize list of words and join\n",
        "  text = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
        "\n",
        "  # Me quedo solo con caracteres basicos y numeros y los reemplazo por espacios\n",
        "  text = \" \".join([re.sub('[^A-Za-z?!]+', ' ', a ) for a in text.split()  ])\n",
        "\n",
        "  # Reemplazo multiples \"exclamation mark\" con uno\n",
        "  # text = re.sub(\"\\?{2,}\", \"??\",text)\n",
        "  # text = re.sub(\"!{2,}\" , \"!!\",text)\n",
        "  # text=text.replace('??' , '?')\n",
        "\n",
        "\n",
        "\n",
        "  # Remueve palabras de 1 caracter ya que seguro son articulos o nonsense\n",
        "  text = \" \".join( [ a for a in text.split() if len(a)> 1 ])\n",
        "\n",
        "  #Como en la cadena anterior pude haber sumado muchos espacios los remplazo por uno solo\n",
        "  text = ' '.join(text.split())\n",
        "\n",
        "\n",
        "  # Transformo el texto a minuscula\n",
        "  text = text.lower()\n",
        "\n",
        "   \n",
        "  # Reemplazo 2 o mas caracteres iguales por solo dos caracteres\n",
        "  text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n",
        "  text = re.sub(r'(\\W)\\1{2,}', r'\\1\\1', text)\n",
        "\n",
        "\n",
        "  return text\n",
        "\n",
        "\n",
        "clean_text('MaaaaaxiNahuelRinaldi!!!!!!!!!!!!???????okwx ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'maaxi nahuel rinaldi okwx'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "stream",
          "text": [
            "time: 1.46 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YaigIKY7bj_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cee6b885-6d6f-4161-c74a-d2023e8c098d"
      },
      "source": [
        "df_clean_test['text_clean'] = ''\n",
        "df_clean_train['text_clean'] = ''\n",
        "df_clean_predict['text_clean'] = ''\n",
        "\n",
        "def fn_df_clean_text(df_clean_text):\n",
        "  df_clean_text.text_clean = df_clean_text.text.apply( lambda x: clean_text(x) )\n",
        "  return df_clean_text\n",
        "\n",
        "\n",
        "df_split_train = np.array_split(df_clean_train, n_cores )\n",
        "df_split_test  = np.array_split(df_clean_test, n_cores )\n",
        "df_split_predict  = np.array_split(df_clean_predict, n_cores )\n",
        "\n",
        "pool = Pool(n_cores)\n",
        "df_clean_train = pd.concat(pool.map(fn_df_clean_text, df_split_train))\n",
        "df_clean_test  = pd.concat(pool.map(fn_df_clean_text, df_split_test))\n",
        "df_clean_predict = pd.concat(pool.map(fn_df_clean_text, df_split_predict))\n",
        "\n",
        "\n",
        "\n",
        "pool.close()\n",
        "pool.join()\n",
        "\n",
        "# df_clean_text.text = df_clean_text.text.apply( lambda x: clean_text(x) )\n",
        "\n",
        "# df_clean_text.text\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.42 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-98nwS7BgMZd",
        "colab_type": "text"
      },
      "source": [
        "# Embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxqYNbBTfkeY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1b33d67c-52fa-414b-f20b-e3af8e4db0e8"
      },
      "source": [
        "from tqdm import tqdm\n",
        "embedding_vector = {}\n",
        "# f = open('/home/rinal/Desktop/nlp_maxi/glove.6B.50d.txt')\n",
        "f = open('/home/rinal/Desktop/nlp_maxi/glove.twitter.27B.50d.txt')\n",
        "\n",
        "_EMBEDDINGS_MATRIX_N_COLS_ = False\n",
        "for line in tqdm(f):\n",
        "\tvalue = line.split(' ')\n",
        "\tword = value[0]\n",
        "\tcoef = np.array(value[1:],dtype = 'float32')\n",
        "\tembedding_vector[word] = coef\n",
        "\tif _EMBEDDINGS_MATRIX_N_COLS_ == False: _EMBEDDINGS_MATRIX_N_COLS_ = len(coef)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1193517it [00:18, 66109.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time: 18.1 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM4CP750wDLK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "452de1e2-c1c2-42e2-ee68-9bddcec0db78"
      },
      "source": [
        "embedding_multi_word_separator = '-'\n",
        "\n",
        "multi_word_expression_list = []\n",
        "for x in list(embedding_vector.keys()):\n",
        "  if x.count(embedding_multi_word_separator): multi_word_expression_list.append(x)\n",
        "\n",
        "# len( multi_word_expression_list)\n",
        "\n",
        "# print(len(multi_word_expression_list), multi_word_expression_list[1000:1010])\n",
        "\n",
        "#Armo un nuevo spliter con el separato middle-score que es el utilizado por gloe\n",
        "from nltk.tokenize import MWETokenizer\n",
        "tokenizer = MWETokenizer(separator=embedding_multi_word_separator )\n",
        "\n",
        "# [3].split('-')\n",
        "for x in multi_word_expression_list:\n",
        "  splitted_sentence = x.split(embedding_multi_word_separator)\n",
        "  if len(splitted_sentence) > 1:\n",
        "    tokenizer.add_mwe(splitted_sentence)\n",
        "\n",
        "# splitted_sentence\n",
        "# tokenizer.tokenize('Something about the west wing'.split())\n",
        "\n",
        "tokenizer.tokenize('a lot of this in a place closer to save a lot'.split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a-lot', 'of', 'this', 'in', 'a', 'place', 'closer', 'to', 'save', 'a-lot']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "stream",
          "text": [
            "time: 278 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv-06RTXtxmo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "32111a99-6cda-48a4-a4be-0017bb455f4a"
      },
      "source": [
        "_EMBEDDINGS_MATRIX_N_COLS_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "stream",
          "text": [
            "time: 1.53 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhuTs82df5Bf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "61b428b2-ecd9-4a34-90f7-587562297e98"
      },
      "source": [
        "phrase_max_words = 35\n",
        "\n",
        "\n",
        "# from nltk.tokenize import MWETokenizer\n",
        "\n",
        "def phrase_to_embedding_matrix( embedding_vector , test_phrase, max_words , emb_n_cols):\n",
        "  res = []\n",
        "  # tokenizer.tokenize('a lot of this in a place closer to save a lot'.split())\n",
        "  for word in test_phrase.split():\n",
        "  # for word in tokenizer.tokenize(test_phrase.split()):\n",
        "    embedding_value = embedding_vector.get(word)\n",
        "    if embedding_value is not None:\n",
        "      res.append( embedding_value )\n",
        "  # Completo para tener todas matrices de embeddings de igual dimension\n",
        "  for x in range(0, max_words - len(res)):\n",
        "    res.append( np.asarray([0 for x in range(0,emb_n_cols)]) )\n",
        "  # return res\n",
        "\n",
        "  return np.array(res[0:max_words])\n",
        " \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.52 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjrIphVOg9VB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "28b55755-196a-4748-a20d-93291716dc02"
      },
      "source": [
        "test_phrase = \"hello my name is Juan asdf asdf asdf asdfasdf adsf  asdf adfs asdf sdfa dafs dfas afd s dfs dfsa asdf asdf a asdf aa aasdf adsf asdf asdf asdf asdf asdf adsf adsf asdf sadf sdaf sdfa asfd asdf sdf a adfs dsaf sdfa dsf dfsa dfsadfsa dfa s dsaf sdfa\"\n",
        "a = phrase_to_embedding_matrix( embedding_vector , test_phrase,  phrase_max_words, _EMBEDDINGS_MATRIX_N_COLS_ )\n",
        "# a\n",
        "# np.asarray(a).shape\n",
        "print( type(a) , a.shape ) \n",
        "\n",
        "\n",
        "# [0,1,2,4][0:2+1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> (35, 50)\n",
            "time: 1.38 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtI-6nNkwibu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5c97fbcd-f4a6-48bb-8a41-cfcc382ebf8a"
      },
      "source": [
        "# Para comprobar si una palabara está en el vocabulario\n",
        "# 'level' in model.wv.vocab\n",
        "\n",
        "\n",
        "# Vector de entrenamiento de la CNN\n",
        "df_clean_train['emb'] = df_clean_train.text_clean.apply( lambda x: phrase_to_embedding_matrix(embedding_vector, x,  phrase_max_words, _EMBEDDINGS_MATRIX_N_COLS_ ) )\n",
        "\n",
        "\n",
        "# Vector de cross validation de la CNN\n",
        "df_clean_test['emb'] = df_clean_test.text_clean.apply( lambda x: phrase_to_embedding_matrix(embedding_vector, x,  phrase_max_words, _EMBEDDINGS_MATRIX_N_COLS_ ) )\n",
        "\n",
        "# Vector con muestras a predecir\n",
        "df_clean_predict['emb'] = df_clean_predict.text_clean.apply( lambda x: phrase_to_embedding_matrix(embedding_vector, x,  phrase_max_words, _EMBEDDINGS_MATRIX_N_COLS_ ) )\n",
        "\n",
        "# df_clean_test.emb.apply(lambda x: len(x)).describe()\n",
        "# df_clean_train.emb.apply(lambda x: len(x)).describe()\n",
        "\n",
        "\n",
        "# np.array(df_clean_test.emb.copy().to_list()).shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.72 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbFyHqZcIxS_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "ee69494f-9594-44e2-db26-519324f8fbc3"
      },
      "source": [
        "df_clean_predict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id keyword location  \\\n",
              "0         0     NaN      NaN   \n",
              "1         2     NaN      NaN   \n",
              "2         3     NaN      NaN   \n",
              "3         9     NaN      NaN   \n",
              "4        11     NaN      NaN   \n",
              "...     ...     ...      ...   \n",
              "3258  10861     NaN      NaN   \n",
              "3259  10865     NaN      NaN   \n",
              "3260  10868     NaN      NaN   \n",
              "3261  10874     NaN      NaN   \n",
              "3262  10875     NaN      NaN   \n",
              "\n",
              "                                                   text  \\\n",
              "0                    Just happened a terrible car crash   \n",
              "1     Heard about #earthquake is different cities, s...   \n",
              "2     there is a forest fire at spot pond, geese are...   \n",
              "3              Apocalypse lighting. #Spokane #wildfires   \n",
              "4         Typhoon Soudelor kills 28 in China and Taiwan   \n",
              "...                                                 ...   \n",
              "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...   \n",
              "3259  Storm in RI worse than last hurricane. My city...   \n",
              "3260  Green Line derailment in Chicago http://t.co/U...   \n",
              "3261  MEG issues Hazardous Weather Outlook (HWO) htt...   \n",
              "3262  #CityofCalgary has activated its Municipal Eme...   \n",
              "\n",
              "                                             text_clean  \\\n",
              "0                      just happened terrible car crash   \n",
              "1     heard about earthquake is different city stay ...   \n",
              "2     there is forest fire at spot pond goose are fl...   \n",
              "3                  apocalypse lighting spokane wildfire   \n",
              "4             typhoon soudelor kill in china and taiwan   \n",
              "...                                                 ...   \n",
              "3258  earthquake safety los angeles safety fasteners...   \n",
              "3259  storm in ri worse than last hurricane my city ...   \n",
              "3260                   green line derailment in chicago   \n",
              "3261            meg issue hazardous weather outlook hwo   \n",
              "3262  cityof calgary ha activated it municipal emerg...   \n",
              "\n",
              "                                                    emb  \n",
              "0     [[0.07746600359678268, 0.37777000665664673, 0....  \n",
              "1     [[0.05573499947786331, 1.55239999294281, 0.693...  \n",
              "2     [[0.2671400010585785, 0.5159500241279602, 0.07...  \n",
              "3     [[0.12394999712705612, 0.6825799942016602, -1....  \n",
              "4     [[-0.4386099874973297, 0.8418300151824951, -0....  \n",
              "...                                                 ...  \n",
              "3258  [[-0.14505000412464142, 1.4453999996185303, 0....  \n",
              "3259  [[-0.05170400068163872, 0.09736700356006622, -...  \n",
              "3260  [[-0.17132000625133514, -1.871399998664856, 0....  \n",
              "3261  [[0.009863300248980522, 0.10206999629735947, -...  \n",
              "3262  [[0.24959999322891235, 0.2643299996852875, -0....  \n",
              "\n",
              "[3263 rows x 6 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>emb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "      <td>just happened terrible car crash</td>\n",
              "      <td>[[0.07746600359678268, 0.37777000665664673, 0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "      <td>heard about earthquake is different city stay ...</td>\n",
              "      <td>[[0.05573499947786331, 1.55239999294281, 0.693...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "      <td>there is forest fire at spot pond goose are fl...</td>\n",
              "      <td>[[0.2671400010585785, 0.5159500241279602, 0.07...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "      <td>apocalypse lighting spokane wildfire</td>\n",
              "      <td>[[0.12394999712705612, 0.6825799942016602, -1....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "      <td>typhoon soudelor kill in china and taiwan</td>\n",
              "      <td>[[-0.4386099874973297, 0.8418300151824951, -0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3258</th>\n",
              "      <td>10861</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
              "      <td>earthquake safety los angeles safety fasteners...</td>\n",
              "      <td>[[-0.14505000412464142, 1.4453999996185303, 0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3259</th>\n",
              "      <td>10865</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
              "      <td>storm in ri worse than last hurricane my city ...</td>\n",
              "      <td>[[-0.05170400068163872, 0.09736700356006622, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3260</th>\n",
              "      <td>10868</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
              "      <td>green line derailment in chicago</td>\n",
              "      <td>[[-0.17132000625133514, -1.871399998664856, 0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3261</th>\n",
              "      <td>10874</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
              "      <td>meg issue hazardous weather outlook hwo</td>\n",
              "      <td>[[0.009863300248980522, 0.10206999629735947, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3262</th>\n",
              "      <td>10875</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
              "      <td>cityof calgary ha activated it municipal emerg...</td>\n",
              "      <td>[[0.24959999322891235, 0.2643299996852875, -0....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3263 rows × 6 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "stream",
          "text": [
            "time: 166 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p26JWqEvxE04",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "4f718ef5-b448-4b9d-cafe-a0770c7bf98f"
      },
      "source": [
        "# valido que todas las secuencias de palabaras tengan largo max_words( en este caso 35)\n",
        "df_clean_train.emb.apply(lambda x: len(x)).describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    7613.0\n",
              "mean       35.0\n",
              "std         0.0\n",
              "min        35.0\n",
              "25%        35.0\n",
              "50%        35.0\n",
              "75%        35.0\n",
              "max        35.0\n",
              "Name: emb, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "stream",
          "text": [
            "time: 10.2 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r6u9iMBx8UO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "44e62d9c-834f-4f1d-b224-31285292518a"
      },
      "source": [
        "# \n",
        "np.asarray(   df_clean_test.emb.copy().to_list() ).shape\n",
        "\n",
        " # Para obtener algo como esto (1523, 35, 50)\n",
        " # se requiere que todas las matrices sean exactametne de la misma foram"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1523, 35, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "stream",
          "text": [
            "time: 6.76 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afFGk2j6Njlc",
        "colab_type": "text"
      },
      "source": [
        "# Modelo Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x803CD88Nf7j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "ddc720ee-2577-4b0c-f698-34d58095181a"
      },
      "source": [
        "# import tensorflow as tf\n",
        "from keras import models\n",
        "from keras import layers\n",
        "# https://missinglink.ai/guides/keras/keras-conv1d-working-1d-convolutional-neural-networks-keras/\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/home/rinal/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/home/rinal/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/home/rinal/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/home/rinal/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/home/rinal/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/home/rinal/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time: 2.54 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/home/rinal/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/home/rinal/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/home/rinal/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/home/rinal/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/home/rinal/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/home/rinal/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76ktflJjR9M7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "21fda2b5-5b10-4a3a-af4d-ccaf9950806b"
      },
      "source": [
        "\n",
        "\n",
        "# tf.keras.utils.to_categorical([0,1,2])\n",
        "def load_dataset():\n",
        "\n",
        "  # Paso el embedding a list y luego a numpy array\n",
        "  train_x = np.asarray(df_clean_train.emb.copy().to_list( ) )\n",
        "  # Devuelve una lista\n",
        "  train_y = tf.keras.utils.to_categorical(df_clean_train.target )\n",
        "\n",
        "  test_x = np.asarray(df_clean_test.emb.copy().to_list())\n",
        "  test_y = tf.keras.utils.to_categorical(df_clean_test.target )\n",
        "\n",
        "  predict_x = np.asarray(df_clean_predict.emb.copy().to_list())\n",
        "\n",
        "  # return train_x\n",
        "  return train_x,train_y, test_x, test_y, predict_x\n",
        "\n",
        "# trainX  = load_dataset()\n",
        "\n",
        "\n",
        "trainX, trainY, testX, testY, predictX = load_dataset()\n",
        "\n",
        "print(  type(trainX) , trainX.shape)\n",
        "print(  type(trainY) , trainY.shape )\n",
        "print(  type(testX)  , testX.shape )\n",
        "print(  type(testY) ,  testY.shape )\n",
        "print(  type(predictX) ,  predictX.shape )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> (7613, 35, 50)\n",
            "<class 'numpy.ndarray'> (7613, 2)\n",
            "<class 'numpy.ndarray'> (1523, 35, 50)\n",
            "<class 'numpy.ndarray'> (1523, 2)\n",
            "<class 'numpy.ndarray'> (3263, 35, 50)\n",
            "time: 60.5 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWM0pcVpXvzZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3d048b18-8764-4cbe-d7df-8e9dc5216f49"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.36 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fXNlrLFs8dQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8061c5ec-8b84-46e7-dda0-4f71050e3191"
      },
      "source": [
        "# !pip3 install pydot\n",
        "# !pip install pydot\n",
        "\n",
        "# import pydot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 409 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCjifVVgOCh-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7c804d08-6af9-42dd-f394-4f18ea221692"
      },
      "source": [
        "import tensorflow\n",
        "import keras\n",
        "session_conf = tensorflow.ConfigProto(intra_op_parallelism_threads=7, inter_op_parallelism_threads=8)\n",
        "tensorflow.set_random_seed(1)\n",
        "sess = tensorflow.Session(graph=tensorflow.get_default_graph(), config=session_conf)\n",
        "keras.backend.set_session(sess)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 278 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZhuegLvOzml",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "4c77f285-e4c0-4152-ef76-15f2ff68731f"
      },
      "source": [
        "import numpy as np\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "# from keras.layers import Conv1D, Dense, MaxPool1D, Flatten, Dropout\n",
        "from keras.layers import *\n",
        "from numpy import mean, std\n",
        "\n",
        "# activation functions https://keras.io/api/layers/activations/\n",
        "\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "def create_model(trainX, trainY, testX, testY):\n",
        "\tn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainY.shape[1]\n",
        "\tmodel = Sequential()\n",
        "\t\n",
        "\t#  kernel_size\n",
        "\t# filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
        "\tmodel.add(Conv1D(filters=20, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
        "\tmodel.add(Conv1D(filters=20, kernel_size=3, activation='relu'))\n",
        "\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(MaxPooling1D(pool_size=2))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(100, activation='relu'))\n",
        "\tmodel.add(Dense(n_outputs, activation='softmax'))\n",
        "\t# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc',f1_m,precision_m, recall_m])\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        " \n",
        "\treturn model\n",
        "\n",
        "\n",
        "def summarize_results(scores):\n",
        "\tprint(scores)\n",
        "\tm, s = mean(scores), std(scores)\n",
        "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
        " \n",
        "# run an experiment\n",
        "def run_experiment(repeats=3):\n",
        "\t# load data\n",
        "\ttrainX, trainY, testX, testY, predictX = load_dataset()\n",
        "\t# repeat experiment\t\n",
        "\n",
        "\tcnn_model = create_model(trainX, trainY, testX, testY)\n",
        " \n",
        "\tcnn_model.summary()\n",
        " \n",
        "\t#  https://machinelearningmastery.com/visualize-deep-learning-neural-network-model-keras/#:~:text=Keras%20provides%20a%20way%20to,output%20shape%20of%20each%20layer.\n",
        "\tplot_model(cnn_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "\tscores = list()\n",
        "\tfor r in range(repeats):\n",
        "\t\tcnn_model = create_model(trainX, trainY, testX, testY)\n",
        "\n",
        "\t\tverbose, epochs, batch_size = 0, 30, 32\n",
        "\t\t# n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainY.shape[1]\n",
        "\n",
        "\t\t# fit network\n",
        "\t\tcnn_model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "\t\t# evaluate model\n",
        "\t\t# loss_value, accuracy,f1_score,precision, recall = cnn_model.evaluate(testX, testY, batch_size=batch_size, verbose=0)\n",
        "\t\tloss_value, accuracy = cnn_model.evaluate(testX, testY, batch_size=batch_size, verbose=0)\n",
        "\t\n",
        "\t\taccuracy = accuracy * 100.0\n",
        "\t\tprint('>#%d: accuracy: %.3f' % (r+1, accuracy))\n",
        "\t\t# print('>#%d: f1_score:  %.3f' % (r+1, f1_score))\n",
        "\t\t# print('>#%d: precission:  %.3f' % (r+1, precision))\n",
        "\t\t# print('>#%d: recall:  %.3f' % (r+1, recall))\n",
        "\t\tresult_file = \"005_embedding_glove_iter_%03d.keras\" % r\n",
        "\t\tcnn_model.save(result_file)\n",
        "\n",
        "\t\t# scores.append([result_file, accuracy,f1_score,precision,recall ] )\n",
        "\t\tscores.append([result_file, accuracy ] )\n",
        "\t\n",
        "\treturn scores\n",
        "\t\n",
        "\t# predicted_y = cnn_model.predict( predictX )\n",
        " \n",
        "\t# return predicted_y\n",
        " \n",
        "\t\t# scores.append(score)\n",
        "\t# summarize results\n",
        "\t# summarize_results(scores)\n",
        " \n",
        "# run the experiment\n",
        "scores = run_experiment(2)\n",
        "\n",
        "scores\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_47 (Conv1D)           (None, 33, 20)            3020      \n",
            "_________________________________________________________________\n",
            "conv1d_48 (Conv1D)           (None, 31, 20)            1220      \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 31, 20)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_24 (MaxPooling (None, 15, 20)            0         \n",
            "_________________________________________________________________\n",
            "flatten_24 (Flatten)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 34,542\n",
            "Trainable params: 34,542\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            ">#1: accuracy: 92.383\n",
            ">#2: accuracy: 94.485\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['005_embedding_glove_iter_000.keras', 92.38345623016357],\n",
              " ['005_embedding_glove_iter_001.keras', 94.48456764221191]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        },
        {
          "output_type": "stream",
          "text": [
            "time: 1min 3s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu27au8lJHbO",
        "colab_type": "text"
      },
      "source": [
        "# Scores "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIgdIrueCVwU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "4b5c787c-0c92-4b6b-a610-ba0a70a50792"
      },
      "source": [
        "# sorted([['a' , 1 ], ['b', 3], ['c' ,2] ] , key = lambda x: x[1] )[0][0]\n",
        "\n",
        "sorted_scores = sorted( scores , key = lambda x: x[1] , reverse=True)\n",
        "\n",
        "best_model = models.load_model(sorted_scores[0][0])\n",
        "\n",
        "trainX, trainY, testX, testY, predictX = load_dataset()\n",
        "predicted_classes = best_model.predict_classes(predictX )\n",
        "\n",
        "print(sorted_scores)\n",
        "\n",
        "print(sum(predicted_classes)/ len(predicted_classes))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['005_embedding_glove_iter_001.keras', 94.48456764221191], ['005_embedding_glove_iter_000.keras', 92.38345623016357]]\n",
            "0.37695372356726936\n",
            "time: 6.17 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL3UxR7QGiE1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "04ad8b6b-c8bd-4e5a-b733-ab4d579ad77c"
      },
      "source": [
        "\n",
        "sorted_scores[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['005_embedding_glove_iter_001.keras', 94.48456764221191]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        },
        {
          "output_type": "stream",
          "text": [
            "time: 2.76 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjxMocMPJsLi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "22ed316a-50f7-469c-9814-90ec4b658309"
      },
      "source": [
        "sum(df_clean_train.target.values) / len(df_clean_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4296597924602653"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        },
        {
          "output_type": "stream",
          "text": [
            "time: 3.88 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK7yEfUOH7X5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9a159755-1d06-4b7e-82c9-ac16a4bff059"
      },
      "source": [
        "print( trainX.shape , predictX.shape )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7613, 35, 50) (3263, 35, 50)\n",
            "time: 1.14 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ifW-6R878Ki",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9d974b65-2f2d-4f79-8d4f-b50f70a19e14"
      },
      "source": [
        "# len(df_clean_predict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 798 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh1Fz9hCK-V3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e8e7da86-752f-49ac-84d3-d515410bf5a7"
      },
      "source": [
        "#  Si target esta en las columnas es porque estoy en el caso de cross validation\n",
        "if 'target' in  df_clean_predict.columns:\n",
        "  df_clean_predict['target_by_embedding'] = predicted_classes\n",
        "  df_clean_predict['target_by_embedding'].value_counts()\n",
        "  df_clean_predict\n",
        "else:\n",
        "  # Si target no esta en las columnas es porque estoy en el caso del set de TEST\n",
        "  df_clean_predict['target'] = predicted_classes\n",
        "  df_clean_predict['target'].value_counts()\n",
        "  df_clean_predict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 4.13 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_TYW1sm7-3C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "6b8f9246-d0aa-4f56-8dbb-1177f6dfd969"
      },
      "source": [
        "df_clean_predict.head(n=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
              "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
              "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
              "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
              "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
              "5  12     NaN      NaN                 We're shaking...It's an earthquake   \n",
              "6  21     NaN      NaN  They'd probably still show more life than Arse...   \n",
              "7  22     NaN      NaN                                  Hey! How are you?   \n",
              "8  27     NaN      NaN                                   What a nice hat?   \n",
              "9  29     NaN      NaN                                          Fuck off!   \n",
              "\n",
              "                                          text_clean  \\\n",
              "0                   just happened terrible car crash   \n",
              "1  heard about earthquake is different city stay ...   \n",
              "2  there is forest fire at spot pond goose are fl...   \n",
              "3               apocalypse lighting spokane wildfire   \n",
              "4          typhoon soudelor kill in china and taiwan   \n",
              "5                     we re shaking it an earthquake   \n",
              "6  they probably still show more life than arsena...   \n",
              "7                                    hey how are you   \n",
              "8                                      what nice hat   \n",
              "9                                           fuck off   \n",
              "\n",
              "                                                 emb  target  \\\n",
              "0  [[0.07746600359678268, 0.37777000665664673, 0....       1   \n",
              "1  [[0.05573499947786331, 1.55239999294281, 0.693...       1   \n",
              "2  [[0.2671400010585785, 0.5159500241279602, 0.07...       1   \n",
              "3  [[0.12394999712705612, 0.6825799942016602, -1....       1   \n",
              "4  [[-0.4386099874973297, 0.8418300151824951, -0....       1   \n",
              "5  [[0.7005699872970581, 1.242900013923645, 0.143...       0   \n",
              "6  [[0.49476000666618347, 0.6372100114822388, -0....       0   \n",
              "7  [[0.7680000066757202, 0.7708799839019775, -0.3...       0   \n",
              "8  [[0.7144100069999695, 0.46241000294685364, 0.0...       0   \n",
              "9  [[1.1220999956130981, 0.2868199944496155, -0.2...       0   \n",
              "\n",
              "   target_by_embedding  \n",
              "0                    1  \n",
              "1                    1  \n",
              "2                    1  \n",
              "3                    1  \n",
              "4                    1  \n",
              "5                    0  \n",
              "6                    0  \n",
              "7                    0  \n",
              "8                    0  \n",
              "9                    0  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>emb</th>\n",
              "      <th>target</th>\n",
              "      <th>target_by_embedding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "      <td>just happened terrible car crash</td>\n",
              "      <td>[[0.07746600359678268, 0.37777000665664673, 0....</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "      <td>heard about earthquake is different city stay ...</td>\n",
              "      <td>[[0.05573499947786331, 1.55239999294281, 0.693...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "      <td>there is forest fire at spot pond goose are fl...</td>\n",
              "      <td>[[0.2671400010585785, 0.5159500241279602, 0.07...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "      <td>apocalypse lighting spokane wildfire</td>\n",
              "      <td>[[0.12394999712705612, 0.6825799942016602, -1....</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "      <td>typhoon soudelor kill in china and taiwan</td>\n",
              "      <td>[[-0.4386099874973297, 0.8418300151824951, -0....</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>12</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>We're shaking...It's an earthquake</td>\n",
              "      <td>we re shaking it an earthquake</td>\n",
              "      <td>[[0.7005699872970581, 1.242900013923645, 0.143...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>They'd probably still show more life than Arse...</td>\n",
              "      <td>they probably still show more life than arsena...</td>\n",
              "      <td>[[0.49476000666618347, 0.6372100114822388, -0....</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>22</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hey! How are you?</td>\n",
              "      <td>hey how are you</td>\n",
              "      <td>[[0.7680000066757202, 0.7708799839019775, -0.3...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>27</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What a nice hat?</td>\n",
              "      <td>what nice hat</td>\n",
              "      <td>[[0.7144100069999695, 0.46241000294685364, 0.0...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>29</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fuck off!</td>\n",
              "      <td>fuck off</td>\n",
              "      <td>[[1.1220999956130981, 0.2868199944496155, -0.2...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        },
        {
          "output_type": "stream",
          "text": [
            "time: 169 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUVCo8GP376w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "71d7bdd7-ea33-4d29-d54a-8738b73f73e3"
      },
      "source": [
        "\n",
        "df_clean_predict['target'] =predicted_classes\n",
        "# df_clean_predict['target_by_embedding'] = predicted_classes\n",
        "out_file_name = 'MAXI_005_TEST_EMBEDDING_GLOVE_SINGLE_WORD_TOKENIZER_FILTER_20_KERNEL_3_DROPOUT_05_EPOCHS_30_REPEAT_2.CSV'\n",
        "df_clean_predict[['id','target']].to_csv(out_file_name,header=True,index = False)\n",
        "\n",
        "\n",
        "# if 'target_by_embedding' in df_clean_predict.columns:\n",
        "#   out_file_name = 'MAXI_005_CROSS_VAL_EMBEDDING_GLOVE_SINGLE_WORD_TOKENIZER_FILTER_10_KERNEL_2_DROPOUT_05.CSV'\n",
        "#   df_clean_predict[['id','target_by_embedding']].to_csv(out_file_name,header=True,index = False)\n",
        "# else:\n",
        "#   out_file_name = 'MAXI_005_TEST_EMBEDDING_GLOVE_SINGLE_WORD_TOKENIZER_FILTER_20_KERNEL_3_DROPOUT_05_EPOCHS_50_REPEAT_2.CSV'\n",
        "#   df_clean_predict[['id','target']].to_csv(out_file_name,header=True,index = False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 28.4 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHmaSbw0NUsP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "02dee647-4308-4168-e235-b3fd92cf34ab"
      },
      "source": [
        "# from tensorflow.python.client import device_lib\n",
        "# print(device_lib.list_local_devices())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.16 ms\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}